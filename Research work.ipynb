{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Импортирование модуля Pandas, присвоение ему краткого имени pd.\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Импортирование исходного датасета, присвоение названия каждой колонке.\n",
    "df = pd.read_csv('ds160216.csv', header = None, sep=',')\n",
    "df.columns = ['uuid', 'short', 'long', 'label',  'date', 'source']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Подсчет количества уникальных групп лейблов новостей, исключая новости с лейблами \"-\", \"S\" и \"Standard\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Способ первый\n",
    "num = 0\n",
    "label_num = dict()\n",
    "#Храним в словаре все лейблы, кроме \"-\", \"S\" и \"Standard \"\n",
    "for label in df.label:\n",
    "    if label != '-' and label != 'S' and label != 'Standard ':\n",
    "        label_num[label] = 0\n",
    "#Считаем количество записей в словаре\n",
    "for label in label_num:\n",
    "    num += 1\n",
    "num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Способ второй\n",
    "pd.Series.nunique(df['label']) - 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Подсчет количества количества уникальных групп новостей, которые содержат более одной новости"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 0\n",
    "label_num = dict()\n",
    "#Храним в словаре все лейблы, кроме \"-\", \"S\" и \"Standard \" \n",
    "for label in df.label:\n",
    "    if label != '-' and label != 'S' and label != 'Standard ':\n",
    "        label_num[label] = 0\n",
    "#Считаем, сколько раз встречается каждый лейбл в исходном датасете\n",
    "for label in df.label:\n",
    "    if label != '-' and label != 'S' and label != 'Standard ':\n",
    "        label_num[label] += 1\n",
    "#Считаем, сколько записей в словаре удовлетворяют условию, что количество новостей в группе больше одного\n",
    "for label in label_num:\n",
    "    if label_num[label] > 1:\n",
    "        num += 1\n",
    "num\n",
    "#Эта процедура необходима для того, чтобы узнать, что существует необходимое для обучения классификатора количество\n",
    "#новостей-дубликатов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Построение гистограммы количества новостей в группе (группа с лейблами \"-\" не включена в гистограмму)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "news_num_hist = pd.Series.value_counts(df.label)[1:]\n",
    "print(news_num_hist.plot.hist(title = u\"Гистограмма количества новостей в группе\", logy = True))\n",
    "#Гистограмма показывает количество случаев того, сколько лейбл содержит новостей. \n",
    "#Так, по гистограмме можно судить, что в датасете содержится огромное число новостей, которые не имеют дубликатов\n",
    "#(что на порядок выше количества лейблов с двумя новостями)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Используя формулу для числа размещений, найдем полное количество пар новостей:\n",
    "#### $C_n^2 = \\binom{n}{2} = \\frac{n!}{2!(n-2)!} = \\frac{n(n-1)}{2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((df.shape[0]) * (df.shape[0] - 1))/2\n",
    "#Результат: 10293613903\n",
    "#Видно, что полное число пар новостей очень велико, проводить обучение классификатора является невозможной задачей\n",
    "#для одного компьютера. В связи с этим попробуем найти количество пар новостей, принадлежащих одной группе, а затем \n",
    "#создать датасет, содержащий равное количество пар новостей, принадлежащих одной группе, и пар новостей, не принадлежащих одной \n",
    "#одной группе."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создание попарного датасета"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Создание датасета, содержащего пары новостей с одинаковыми лейблами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Создаем датасет label_counts, в котором для каждого лейбла хранится число его повторений \n",
    "#(не берем лейблы \"-\", \"S\" и \"Standard \")\n",
    "label_counts = df.label.value_counts()[3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32980"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Вычисление количества пар новостей, принадлежащих одной группе\n",
    "sum(pd.Series.apply(label_counts, lambda x: x*(x-1)/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Найдем отношение количества пар новостей, принадлежащих одной группе, в полному количеству пар новостей\n",
    "float(sum(pd.Series.apply(label_counts, lambda x: x*(x-1)/2))) / (((df.shape[0]) * (df.shape[0] - 1))/2)\n",
    "#Результат: 3.2039282132379393e-06\n",
    "#Это еще одна причина, по которой обучиться будет невозможно: имеется ничтожно малое по сравнению с полным \n",
    "#количество пар новостей, принадлежащих одной группе. Классификатору будет выгодно говорить, что две новости\n",
    "#не являются дубликатами, так как он будет ошибаться всего лишь в 3.2039282132379393e-06 случаях."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Создание датасета, в котором хранятся индексы одинаковых лейблов\n",
    "#Исключаем из датасета лейблы \"-\", \"S\", \"Standard \", храним получившийся датасет в filtered_df\n",
    "filtered_df = df[~df.label.isin(['-', 'S', 'Standard '])]\n",
    "#Присваиваем label_col только колонку 'label'\n",
    "label_col = filtered_df[['label']]\n",
    "#Создаем колонку 'index', в которой содержатся старые индексы лейблов, для каждого лейбла индекс в датасете обновился\n",
    "label_idx = label_col.reset_index(drop=False)\n",
    "#Группируем по лейблам \n",
    "label_groupby = label_idx.groupby('label')\n",
    "#Каждую запись колонки \"index\" превращаем в список\n",
    "label_indices = label_groupby.apply(lambda x: list(x['index']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Построение пар новостей, принадлежащих одной группе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Применяем itertools.combinations из модуля itertools ко всем индексам лейблов из label_indices\n",
    "import itertools\n",
    "num_comb = pd.Series(label_indices.apply(lambda x: list(itertools.combinations(x, 2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#создаем три списка: в первом списке хранится индекс первой новости, во втором списке - второй новости, а в третьем \n",
    "#списке указано, совпадают ли лейблы этих индексов (так как изначально брались одинаковые лейблы, то для всех пар \n",
    "#индексов все значения списка same будут True)\n",
    "index_0 = []\n",
    "index_1 = []\n",
    "same = []\n",
    "#заполняем каждый список\n",
    "for comb in num_comb:\n",
    "    for index_comb in comb:\n",
    "        index_0.append(index_comb[0])\n",
    "        index_1.append(index_comb[1])\n",
    "        same.append('True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Создаем список, в котором объединяем списки index_0, index_1, same.\n",
    "df_index_pair = [[0] * 3 for m in range(len(index_0))]\n",
    "for k in range(len(index_0)):\n",
    "    df_index_pair[k][0] = index_0[k]\n",
    "    df_index_pair[k][1] = index_1[k]\n",
    "    df_index_pair[k][2] = same[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Преобразуем список в датафрейм \n",
    "df_index_pair = pd.DataFrame.from_records(df_index_pair, columns = ['index_0', 'index_1', 'same'])\n",
    "df_index_pair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Построение пар новостей, не принадлежащих одной группе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cоздаем два объекта, в каждом из которых содержится заданное количество n новостей\n",
    "n = df_index_pair.shape[0]\n",
    "label_0, label_1 = df.label[~df.label.isin(['-', 'S', 'Standard '])].sample(n), df.label[~df.label.isin(['-', 'S', 'Standard '])].sample(n)\n",
    "#создаем два списка, в каждом из которых содержатся индексы выбранных новостей. Для этого для каждого значения k \n",
    "#в промежутке (0, df.shape[0]) проверяем, есть ли объект с таким индексом в созданных объектах. Если их нет и выходит ошибка, нужно\n",
    "#продолжить цикл.\n",
    "index_0 = []\n",
    "index_1 = []\n",
    "for k in range(df.shape[0]):\n",
    "    try:\n",
    "        label_0[k]\n",
    "        index_0.append(k)\n",
    "    except KeyError: \n",
    "        continue\n",
    "for k in range(df.shape[0]):\n",
    "    try: \n",
    "        label_1[k]\n",
    "        index_1.append(k)\n",
    "    except KeyError:\n",
    "        continue\n",
    "#создаем список, в котором содержатся значения 'True' или 'False'. Так как все значения должны быть 'False',\n",
    "#то для каждой пары с 'True' делаем следующее: первую новость пары заменяем на случайно выбранную с помощью функции randint из модуля numpy. \n",
    "#cтрочка same.append('True') создана с целью убедиться в правильности построения датасета.\n",
    "import numpy as np\n",
    "same = []\n",
    "for k in range(n):\n",
    "    if df.label[index_0[k]] == df.label[index_1[k]]:\n",
    "        while df.label[index_0[k]] == df.label[index_1[k]]:\n",
    "            index_0[k] = np.random.randint(df.shape[0])\n",
    "        if df.label[index_0[k]] == df.label[index_1[k]]:\n",
    "            same.append('True')\n",
    "        else:\n",
    "            same.append('False')\n",
    "    else:\n",
    "        same.append('False')\n",
    "#создаем список, в который включаем первый индекс, второй индекс, значения 'True' или 'False'\n",
    "df_index_nonpair = [[0] * 3 for m in range(n)]\n",
    "for k in range(n):\n",
    "    df_index_nonpair[k][0] = index_0[k]\n",
    "    df_index_nonpair[k][1] = index_1[k]\n",
    "    df_index_nonpair[k][2] = same[k]\n",
    "#преобразуем список в DataFrame\n",
    "df_index_nonpair = pd.DataFrame.from_records(df_index_nonpair, columns = ['index_0', 'index_1', 'same'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Объединяем датафреймы с одинаковыми лейблами и разными лейблами\n",
    "result = pd.concat([df_index_pair, df_index_nonpair]).reset_index(drop=True)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение классификатора"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Функция для обучения классификатора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Создание функции для обучения классификатора. На вход поступают колонка со значениями True/False а также датафрейм, \n",
    "#содержащий колонки с признаками\n",
    "def classifier(labels, df_selected):\n",
    "    #преобразовываем значения True/False в массив со значениями 0/1\n",
    "    labels = np.asarray(labels)\n",
    "    #импортируем LabelEncoder\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    le = LabelEncoder()\n",
    "    le.fit(labels)\n",
    "    labels = le.transform(labels)\n",
    "    #преобразовываем df_selected в словарь\n",
    "    df_features = df_selected.to_dict(orient='records')\n",
    "    #импортируем DictVectorizer, проводим векторизацию признаков с сохранением в features\n",
    "    from sklearn.feature_extraction import DictVectorizer\n",
    "    vec = DictVectorizer()\n",
    "    features = vec.fit_transform(df_features).toarray()\n",
    "    #разбиваем выборку на train/test в соотношении 80/20\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    train, test, train_labels, test_labels = train_test_split(\n",
    "      features, labels, \n",
    "      test_size=0.20, random_state=42)\n",
    "\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    #инициализируем классификатор RandomForestClassifier как clf\n",
    "    clf = RandomForestClassifier(random_state = 42)\n",
    "    #обучаем классификатор\n",
    "    clf.fit(train, train_labels)\n",
    "    #сохраняем в acc_test точность предсказания\n",
    "    acc_test = clf.score(test, test_labels)\n",
    "    #возвращаем точность предсказания \n",
    "    #последняя строка показывает долю правильных ответов алгоритма.\n",
    "    return \"Test Accuracy:\", acc_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение классификатора без приведения слов к начальной форме"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Токенизация текста без приведения слов к начальной форме"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Импортируем библиотеку nltk, а также стоп-слова\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "#Создадим функцию токенизации текста без приведения слов к начальной форме\n",
    "#принимаем на вход текст\n",
    "def tokenize_me(file_text):\n",
    "    #декодируем текст\n",
    "    file_text = file_text.decode('utf-8')\n",
    "    #Разбиваем текст на токены\n",
    "    tokens = nltk.word_tokenize(file_text)\n",
    "    #все буквы преобразуем в строчные\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    #импортируем библиотеку re, удаляем цифры из токенов\n",
    "    import re\n",
    "    tokens = [re.sub(r'\\d', '', i) for i in tokens]\n",
    "    #удаляем знаки препинания из токенов\n",
    "    tokens = [i for i in tokens if i not in ('!', '', u'`', '.',':', '(', ')', u'№', u'–', u'«', u'»', ',', '...', '-', '[', ']', '{', '}', ';', \"'\", '\"', '<', '>', '/', '?', '@', \"#\", '$', '%', '^', '&', '*', '_')]\n",
    "    stop_words = stopwords.words('russian')\n",
    "    #удаляем стоп-слова \n",
    "    tokens = [i for i in tokens if i not in stop_words]\n",
    "    #удаляем оставшиеся знаки препинания\n",
    "    tokens = [i.replace(u\"`\", \"\").replace(u\"№\", \"\").replace('-', \"\").replace(',', \"\").replace('.', \"\") for i in tokens]\n",
    "    #удаляем токены, содержащие две или меньше букв\n",
    "    tokens = [i for i in tokens if not len(i) <= 2]\n",
    "    #возвращаем токены\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Выделение признаков и обучение классификатора"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Выделение признаков из пар текстов, которые состоят только из short-текстов новостей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#создаем списки, в которых будем хранить short-тексты для каждой новости из пары.\n",
    "dfshort1 = []\n",
    "dfshort2 = []\n",
    "#к dfshort1 добавляем short-тексты первой новости пары (с result.index_0).\n",
    "for k in result.index_0:\n",
    "    dfshort1.append(df.short[k])\n",
    "#к dfshort2 добавляем short-тексты второй новости пары (с result.index_1).\n",
    "for k in result.index_1:\n",
    "    dfshort2.append(df.short[k])\n",
    "#создаем список dfshort_all, в котором будем хранить данные из списков dfshort1 и dfshort2. Добавляем два\n",
    "#дополнительных столбца\n",
    "dfshort_all = [[0] * 4 for i in range(len(dfshort1))]\n",
    "for k in range(len(dfshort1)):\n",
    "    dfshort_all[k][0] = dfshort1[k]\n",
    "    dfshort_all[k][1] = dfshort2[k]\n",
    "    dfshort_all[k][2] = 0\n",
    "    dfshort_all[k][3] = 0\n",
    "#Преобразовываем список dfshort_all в датафрейм с колонками 'index_0', 'index_1', 'same_items', 'same'\n",
    "dfshort_all = pd.DataFrame.from_records(dfshort_all, columns = ['index_0', 'index_1', 'same_items', 'same'])\n",
    "#Значениям колонки dfshort_all.same присваиваем значения колонки result.same\n",
    "dfshort_all.same = result.same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#проводим токенизацию всех текстов из dfshort_all.index_0 и dfshort_all.index_1\n",
    "pd.options.mode.chained_assignment = None\n",
    "dfshort_all.index_0 = dfshort_all.index_0.apply(tokenize_me)\n",
    "dfshort_all.index_1 = dfshort_all.index_1.apply(tokenize_me)\n",
    "#помещаем токены каждого текста в множества\n",
    "dfshort_all.index_0_sets = dfshort_all.index_0.apply(set)\n",
    "dfshort_all.index_1_sets = dfshort_all.index_1.apply(set)\n",
    "#находим число пересечений множеств, содержащих токены первой и второй новостей, присваиваем эти значения колонке same_items\n",
    "for k in range(dfshort_all.shape[0]):\n",
    "    dfshort_all.same_items[k] = len(dfshort_all.index_0_sets[k].intersection(dfshort_all.index_1_sets[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63389"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#посчитаем, сколько признаков было найдено при пересечении множеств, содержащих токены short-текстов первой и второй новостей\n",
    "sum(dfshort_all.same_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Test Accuracy:', 0.88083687083080653)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#обучим классификатор, получим точность\n",
    "classifier(dfshort_all.same, dfshort_all.drop(['index_0', 'index_1', 'same'], axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Выделение признаков из пар текстов, которые состоят только из long-текстов новостей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#создаем списки, в которых будем хранить long-тексты для каждой новости из пары.\n",
    "dflong1 = []\n",
    "dflong2 = []\n",
    "#к dflong1 добавляем long-тексты первой новости пары (с result.index_0).\n",
    "for k in result.index_0:\n",
    "    dflong1.append(df.long[k])\n",
    "#к dflong2 добавляем long-тексты второй новости пары (с result.index_1).\n",
    "for k in result.index_1:\n",
    "    dflong2.append(df.long[k])\n",
    "#создаем список dflong_all, в котором будем хранить данные из списков dflong1 и dflong2. Добавляем два\n",
    "#дополнительных столбца\n",
    "dflong_all = [[0] * 4 for i in range(len(dflong1))]\n",
    "for k in range(len(dflong1)):\n",
    "    dflong_all[k][0] = dflong1[k]\n",
    "    dflong_all[k][1] = dflong2[k]\n",
    "    dflong_all[k][2] = 0\n",
    "    dflong_all[k][3] = 0\n",
    "#Преобразовываем список dflong_all в датафрейм с колонками 'index_0', 'index_1', 'same_items', 'same'\n",
    "dflong_all = pd.DataFrame.from_records(dflong_all, columns = ['index_0', 'index_1', 'same_items', 'same'])\n",
    "#Значениям колонки dfshort_all.same присваиваем значения колонки result.same\n",
    "dflong_all.same = result.same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#проводим токенизацию всех текстов из dflong_all.index_0 и dflong_all.index_1\n",
    "pd.options.mode.chained_assignment = None\n",
    "dflong_all.index_0 = dflong_all.index_0.apply(tokenize_me)\n",
    "dflong_all.index_1 = dflong_all.index_1.apply(tokenize_me)\n",
    "#помещаем токены каждого текста в множества\n",
    "dflong_all.index_0_sets = dflong_all.index_0.apply(set)\n",
    "dflong_all.index_1_sets = dflong_all.index_1.apply(set)\n",
    "#находим число пересечений множеств, содержащих токены первой и второй новостей, присваеваем эти значения колонке same_items.\n",
    "for k in range(dflong_all.shape[0]):\n",
    "    dflong_all.same_items[k] = len(dflong_all.index_0_sets[k].intersection(dflong_all.index_1_sets[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "130817"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#посчитаем, сколько признаков было найдено при пересечении множеств, содержащих токены long-текстов первой и второй новостей\n",
    "sum(dflong_all.same_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Test Accuracy:', 0.89084293511218926)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#обучим классификатор, получим точность\n",
    "classifier(dflong_all.same, dflong_all.drop(['index_0', 'index_1', 'same'], axis = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Выделение признаков из пар текстов, которые включают в себя short- и long-тексты новостей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#создаем списки, в которых будем хранить short- и long-текст для каждой новости из пары.\n",
    "dfshort1 = []\n",
    "dfshort2 = []\n",
    "dflong1 = []\n",
    "dflong2 = []\n",
    "#к dfshort1 и dflong1 добавляем short- и long-тексты первой новости пары (с result.index_0).\n",
    "for k in result.index_0:\n",
    "    dfshort1.append(df.short[k])\n",
    "    dflong1.append(df.long[k])\n",
    "#к dfshort2 и dflong2 добавляем short- и long-тексты второй новости пары (с result.index_1).\n",
    "for k in result.index_1:\n",
    "    dfshort2.append(df.short[k])\n",
    "    dflong2.append(df.long[k])\n",
    "#создаем список dfshort_all, в котором будем хранить данные из списков dfshort1 и dfshort2. Добавляем два\n",
    "#дополнительных столбца\n",
    "dfshort_all = [[0] * 4 for i in range(len(dfshort1))]\n",
    "for k in range(len(dfshort1)):\n",
    "    dfshort_all[k][0] = dfshort1[k]\n",
    "    dfshort_all[k][1] = dfshort2[k]\n",
    "    dfshort_all[k][2] = 0\n",
    "    dfshort_all[k][3] = 0\n",
    "#создаем список dflong_all, в котором будем хранить данные из списков dflong1 и dflong2. Добавляем два\n",
    "#дополнительных столбца\n",
    "dflong_all = [[0] * 4 for i in range(len(dflong1))]\n",
    "for k in range(len(dflong1)):\n",
    "    dflong_all[k][0] = dflong1[k]\n",
    "    dflong_all[k][1] = dflong2[k]\n",
    "    dflong_all[k][2] = 0\n",
    "    dflong_all[k][3] = 0\n",
    "#Преобразовываем списки dfshort_all и dflong_all в датафреймы с колонками 'index_0', 'index_1', 'same_items', 'same'\n",
    "dfshort_all = pd.DataFrame.from_records(dfshort_all, columns = ['index_0', 'index_1', 'same_items', 'same'])\n",
    "dflong_all = pd.DataFrame.from_records(dflong_all, columns = ['index_0', 'index_1', 'same_items', 'same'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#создаем датафреймы df_both1 и df_both2, в которых храним объединенные short- и long-тексты для каждой новости из пары\n",
    "df_both1 = dfshort_all.index_0 + ' ' + dflong_all.index_0\n",
    "df_both2 = dfshort_all.index_1 + ' ' + dflong_all.index_1\n",
    "#создаем датафрем df_both с колонками 'index_0', 'index_1', 'same_items', 'same'. Колонке index_0 присваиваем df_both1,\n",
    "#колонке index_1 присваиваем df_both2, колонке same присваиваем колонку result.same\n",
    "df_both = pd.DataFrame(columns = ['index_0', 'index_1', 'same_items', 'same'])\n",
    "df_both.index_0 = df_both1 \n",
    "df_both.index_1 = df_both2\n",
    "df_both.same_items = 0\n",
    "df_both.same = result.same\n",
    "pd.options.mode.chained_assignment = None\n",
    "#проводим токенизацию всех текстов из index_0 и index_1\n",
    "df_both.index_0 = df_both.index_0.apply(tokenize_me)\n",
    "df_both.index_1 = df_both.index_1.apply(tokenize_me)\n",
    "#помещаем токены каждого текста в множества\n",
    "df_both.index_0_sets = df_both.index_0.apply(set)\n",
    "df_both.index_1_sets = df_both.index_1.apply(set)\n",
    "#находим число пересечений множеств, содержащих токены первой и второй новостей\n",
    "for k in range(df_both.shape[0]):\n",
    "    df_both.same_items[k] = len(df_both.index_0_sets[k].intersection(df_both.index_1_sets[k]))\n",
    "#перемешиваем строки в df_both\n",
    "#df_both = df_both.reindex(np.random.permutation(df_both.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95262280169799873"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Обучаем классификатор, получаем точность (accuracy)\n",
    "classifier(df_both.same, df_both.drop(['index_0', 'index_1', 'same'], axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение классификатора с приведением слов к начальной форме"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from pymystem3 import Mystem\n",
    "mystem = Mystem(entire_input=False)\n",
    "import re\n",
    "#С учетом особенностей модуля mystem (который удаляет слова, содержащие цифры, из текста), создадим функцию, \n",
    "#которая будет предварительно убирать цифры, стоп-слова и слова,\n",
    "#содержащие две и менее букв, а затем применим анализатор от mystem, который предоставит начальную форму слов и \n",
    "#грамматически охарактеризует их\n",
    "def mystem_combined(file_text):\n",
    "    #декодируем текст\n",
    "    file_text = file_text.decode('utf-8')\n",
    "    #токенизация текста при помощи nltk.word_tokenize (модуль nltk)\n",
    "    tokens = nltk.word_tokenize(file_text)\n",
    "    #все буквы в словах заменяем на строчные\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    #удаляем цифры\n",
    "    tokens = [re.sub(r'\\d', '', i) for i in tokens]\n",
    "    #удаляем стоп-слова\n",
    "    stop_words = stopwords.words('russian')\n",
    "    tokens = [i for i in tokens if i not in stop_words]\n",
    "    #удаляем слова, содержащие две или менее букв\n",
    "    tokens = [i for i in tokens if not len(i) <= 2]\n",
    "    #сливаем образовавшиеся токены в один текст\n",
    "    file_text = ' '.join(tokens)\n",
    "    #применяем анализатор mystem\n",
    "    file_text = mystem.analyze(file_text)\n",
    "    #теперь для каждого слова хранится грамматическая информация, начальная форма и исходный текст. Удалим исходный текст\n",
    "    #за ненадобностью и в целях уменьшения объема памяти, занимаемого информацией о словах. Так как mystem применим \n",
    "    #только к русскоязычным словам, то для английских слов и нераспознанных русских слов грамматической информации не \n",
    "    #имеется. Поэтому при обращении к ней может возникнуть ошибка IndexError. Будем пропускать такие слова.\n",
    "    for k in range(len(file_text)):\n",
    "        try: \n",
    "            file_text[k][u'analysis'][0][u'gr']\n",
    "            del file_text[k][u'text']\n",
    "        except IndexError:\n",
    "            pass\n",
    "    #создаем новый список, добавляем к нему словари с информацией о словах. Если словари полностью совпадают, то лишний\n",
    "    #словарь не добавляется в список\n",
    "    new_file_text = []\n",
    "    for x in file_text:\n",
    "        if x not in new_file_text:\n",
    "            new_file_text.append(x)\n",
    "    #возвращаем список\n",
    "    return new_file_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение классификатора с выделением признаков: часть речи, \"гео\", \"имя\", \"фам\" и \"отч\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#имеется предварительно созданный файл, в котором каждой части речи присвоено какое-либо значение от 0 до 14, где\n",
    "#цифра 0 обозначает, что часть речи не определена. Это может говорить о том, что слово не русскоязычное либо это слово\n",
    "#не определено словарем (например, из-за ошибок в слове)\n",
    "#импортируем файл characterisics.csv и дадим название его колонкам - word class и code\n",
    "table_ch = pd.read_csv('characteristics.csv', sep=',', header = None)\n",
    "table_ch.columns = ['word class', 'code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#создадим функцию, которая приведет всю информацию о слове к виду: word, x, y, где word - начальная форма слова,\n",
    "#x - код части речи, y - цифра \"0\" - слово не является географическим названием, не является именем, фамилией или отчеством, \n",
    "#цифра \"1\" - слово является географическим названием, цирфа \"2\" - слово является именем, фамилией или отчеством\n",
    "#импортируем модуль string (необходим для нахождения знаков препинания)\n",
    "import string\n",
    "#создадим предварительно вспомогательную функцию geo_name, которая будет часто использоваться в основной функции obrabotka\n",
    "#на вход поступают слово m и список l\n",
    "def geo_name(m, l):\n",
    "    #если слово является географическим названием, к списку l добавить цифру \"1\"\n",
    "    if m == u'гео':\n",
    "        l.append(1)\n",
    "    #если слово является именем, фамилией или отчеством, к списку l добавить цифру \"2\"\n",
    "    elif (m == u\"имя\" or m == u\"фам\" or m == u\"отч\"):\n",
    "        l.append(2)\n",
    "    #если слово не имеет этих признаков, к списку l добавить цифру \"0\"\n",
    "    else:\n",
    "        l.append(0)\n",
    "    #возвращаем l\n",
    "    return l\n",
    "#основная функция, на вход поступает столбец датафрейма\n",
    "def obrabotka(series):\n",
    "    #обращаемся к каждой строке столбца\n",
    "    for p in range(series.shape[0]):\n",
    "        #присваиваем строку line\n",
    "        line = series[p]\n",
    "        #обращаемся к каждому словарю строки, в котором содержится информация о слове\n",
    "        for y in range(len(line)):\n",
    "            #вводим переменные: 1) n - необходим для счета количества обработанных слов в строчке, в которой дается \n",
    "            #грамматическая характеристика слова; это число не должно превышать двух, когда n = 1, это значит,\n",
    "            #что определена часть речи, когда n = 2, это значит, что определена часть речи и признак \"гео\", \"имя\" и т.п.\n",
    "            #2) l - список, который и будет содержать новую информацию о слове; 3) m - слова, здесь происходит \"сборка\"\n",
    "            #слова по буквам\n",
    "            n = 0\n",
    "            l = []\n",
    "            m = ''\n",
    "            #пробуем присоединить к списку l начальную форму слова\n",
    "            try:\n",
    "                l.append(line[y][u'analysis'][0][u'lex'])\n",
    "                #проверяем, не было ли проверено уже два слова из раздела грамматики, если ответ утвердительный, то\n",
    "                #исходная строка заменяется списком l.\n",
    "                for k in range(len(line[y][u'analysis'][0][u'gr'])):\n",
    "                    if n == 2:\n",
    "                        pass\n",
    "                    #в противном случае проверяем, является ли отдельно взятая буква из раздела грамматики знаком препинания\n",
    "                    #(используется модуль string)\n",
    "                    else:\n",
    "                        if line[y][u'analysis'][0][u'gr'][k] in string.punctuation:\n",
    "                            #если это так, то проверяется, является ли следующий символ знаком препинания. Если ответ \n",
    "                            #утвердительный, то ничего не делается (операция необходима для случаев, когда идут подряд\n",
    "                            #два знака препинания. Если пропускать этот шаг, то второй слово из грамматического раздела\n",
    "                            #может быть пропущено, а это значит, что возможно упускается признак)\n",
    "                            #при этом может возникнуть ошибка - IndexError.\n",
    "                            try:\n",
    "                                if line[y][u'analysis'][0][u'gr'][k+1] in string.punctuation:\n",
    "                                    pass\n",
    "                                else:    \n",
    "                                    #если следующий символ не является знаком препинания, то проводим следующие операции\n",
    "                                    #если n = 0, то нужно выделить часть речи\n",
    "                                    if n == 0:\n",
    "                                        l.append(table_ch[table_ch['word class'] == m].index[0])\n",
    "                                    #если n = 1, то нужно попробовать выделить географический или именной признак.\n",
    "                                    #Для этого обращаемся к вспомогательной функции.\n",
    "                                    if n == 1:\n",
    "                                        l = geo_name(m, l)\n",
    "                                    #Значение m обнуляем, чтобы затем наполнить его буквами следующего слова\n",
    "                                    m = ''\n",
    "                                    #значение n увеличиваем на единицу\n",
    "                                    n += 1\n",
    "                            except IndexError:\n",
    "                                #если такая ошибка возникла, то нужно проверить значение n. Если оно равно нулю, то \n",
    "                                #к списку l добавляется код части речи слова. Так как больше информации о слове нет,\n",
    "                                #то и географических и именных признаков нет, поэтому к списку также добавляется число \"0\".\n",
    "                                if n == 0:\n",
    "                                    l.append(table_ch[table_ch['word class'] == m].index[0])\n",
    "                                    l.append(0)\n",
    "                                #если значение n равно единице, то это значит, что часть речи была выделена, и нужно \n",
    "                                #проверить, является ли второе слово географическим или именным признаком. Для этого\n",
    "                                #обращаемся к вспомогательной функции geo_name.\n",
    "                                if n == 1:\n",
    "                                    l = geo_name(m, l)\n",
    "                                #больше информации о слове нет, поэтому приравниваем n к двум.\n",
    "                                n == 2\n",
    "                        else:\n",
    "                            #если следующий символ не явлется знаком препинания, то \"собирание\" слова еще не завершено.\n",
    "                            #Поэтому к m присоединяем символ.\n",
    "                            m += line[y][u'analysis'][0][u'gr'][k]\n",
    "                            try:\n",
    "                                #проверяем, существует ли следующий символ\n",
    "                                line[y][u'analysis'][0][u'gr'][k+1]\n",
    "                            except IndexError:\n",
    "                                #если его не существует, то это значит, что грамматическая информация о слове закончилась.\n",
    "                                #Так как в ней содержится по крайней мере одно слово, то n в данном случае не может быть\n",
    "                                #равен нулю. Значит, часть речи уже выделена, остается попытаться выделить географический\n",
    "                                #или именной признак. Обращаемся к вспомогательной функции geo_name.\n",
    "                                if n == 1:\n",
    "                                    l = geo_name(m, l)\n",
    "                                    n = 2\n",
    "                #После всех операций над символами заменяем исходную строку на список l.\n",
    "                line[y] = l\n",
    "            #если при попытке присоединения к списку l начальной формы слова возникает ошибка IndexError, то\n",
    "            #в список l добавляется исходная форма слова, считается, что никаких признаков слово не имеет\n",
    "            except IndexError:\n",
    "                l.append(line[y][u'text'])\n",
    "                l.append(0)\n",
    "                l.append(0)\n",
    "            #исходную информацию о слове заменяем информацией из списка l\n",
    "                line[y] = l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#После выделения именных и географических признаков переходим к выделению количественных признаков. \n",
    "#импортируем модуль numpy\n",
    "import numpy as np\n",
    "#создаем columns, которому присваиваем значения от 0 до 14\n",
    "columns = [np.arange(15)]\n",
    "#создаем три датафрема с колонками из columns, количество строк определено количеством строк датафрема result\n",
    "df_class1 = pd.DataFrame(columns = columns, index = np.arange(result.shape[0]))\n",
    "df_class2 = pd.DataFrame(columns = columns, index = np.arange(result.shape[0]))\n",
    "df_class3 = pd.DataFrame(columns = columns, index = np.arange(result.shape[0]))\n",
    "#создаем функцию, при выполнении которой получим датафрейм с количеством совпадающих слов двух новостей\n",
    "#по той или иной части речи. На вход поступает колонка датафрейма.\n",
    "def X2_func(x, y):\n",
    "    for k in range(x, y):\n",
    "        p = result.index_0[k]\n",
    "        for m in range(len(dfboth.both[p])):\n",
    "            try:\n",
    "                df_class1[dfboth.both[p][m][1]][k].add(dfboth.both[p][m][0])          \n",
    "            except AttributeError:\n",
    "                df_class1[dfboth.both[p][m][1]][k] = set()\n",
    "                df_class1[dfboth.both[p][m][1]][k].add(dfboth.both[p][m][0])\n",
    "        p1 = result.index_1[k]\n",
    "        for m in range(len(dfboth.both[p1])):\n",
    "            try:\n",
    "                df_class2[dfboth.both[p1][m][1]][k].add(dfboth.both[p1][m][0])            \n",
    "            except AttributeError:\n",
    "                df_class2[dfboth.both[p1][m][1]][k] = set()\n",
    "                df_class2[dfboth.both[p1][m][1]][k].add(dfboth.both[p1][m][0])\n",
    "        for l in range(15):\n",
    "            try:\n",
    "                df_class3[l][k] = len(df_table1[l][k].intersection(df_table2[l][k]))\n",
    "            except AttributeError:\n",
    "                df_class3[l][k] = 0\n",
    "            except TypeError:\n",
    "                df_class3[l][k] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [np.arange(3)]\n",
    "df_table1 = pd.DataFrame(columns = columns, index = np.arange(result.shape[0]))\n",
    "df_table2 = pd.DataFrame(columns = columns, index = np.arange(result.shape[0]))\n",
    "df_table3 = pd.DataFrame(columns = columns, index = np.arange(result.shape[0]))\n",
    "def X3_func(x, y):\n",
    "    for k in range(x, y):\n",
    "        p = result.index_0[k]\n",
    "        for m in range(len(dfboth.both[p])):\n",
    "            try:\n",
    "                df_table1[dfboth.both[p][m][2]][k].add(dfboth.both[p][m][0])          \n",
    "            except AttributeError:\n",
    "                df_table1[dfboth.both[p][m][2]][k] = set()\n",
    "                df_table1[dfboth.both[p][m][2]][k].add(dfboth.both[p][m][0])\n",
    "        p1 = result.index_1[k]\n",
    "        for m in range(len(dfboth.both[p1])):\n",
    "            try:\n",
    "                df_table2[dfboth.both[p1][m][2]][k].add(dfboth.both[p1][m][0])            \n",
    "            except AttributeError:\n",
    "                df_table2[dfboth.both[p1][m][2]][k] = set()\n",
    "                df_table2[dfboth.both[p1][m][2]][k].add(dfboth.both[p1][m][0])\n",
    "        for l in range(3):\n",
    "            try:\n",
    "                df_table3[l][k] = len(df_table1[l][k].intersection(df_table2[l][k]))\n",
    "            except AttributeError:\n",
    "                df_table3[l][k] = 0\n",
    "            except TypeError:\n",
    "                df_table3[l][k] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_class = pd.concat([df_class3, df_table3], axis=1)\n",
    "geo_class['same'] = result.same\n",
    "get_class = geo_class.reindex(np.random.permutation(geo_class.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_class3['same'] = result.same\n",
    "df_table3['same'] = result.same\n",
    "df_class3 = df_class3.reindex(np.random.permutation(df_class3.index))\n",
    "df_table3 = df_table3.reindex(np.random.permutation(df_table3.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier(df_class3.same, df_class3.drop(['same'], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier(df_table3.same, df_table3.drop(['same'], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier(geo_class.same, geo_class.drop(['same'], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
